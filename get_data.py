import json
import os
import re
from datasets import load_dataset
from tqdm import tqdm

def chunk_text(text, chunk_size=256, overlap=32):
    """
    Chia vƒÉn b·∫£n th√†nh c√°c chunk v·ªõi overlap
    
    Args:
        text: VƒÉn b·∫£n c·∫ßn chia
        chunk_size: K√≠ch th∆∞·ªõc m·ªói chunk (s·ªë t·ª´)
        overlap: S·ªë t·ª´ overlap gi·ªØa c√°c chunk
        
    Returns:
        List c√°c chunk
    """
    # T√°ch vƒÉn b·∫£n th√†nh t·ª´
    words = text.split()
    
    if len(words) <= chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(words):
        end = start + chunk_size
        chunk = ' '.join(words[start:end])
        chunks.append(chunk)
        
        # Di chuy·ªÉn v·ªõi overlap, tr·ª´ khi ƒë√£ ƒë·∫øn cu·ªëi
        if end >= len(words):
            break
        start = end - overlap
    
    return chunks

def split_by_sentences(text, chunk_size=256, overlap=32):
    # T√°ch c√¢u ƒë∆°n gi·∫£n (c√≥ th·ªÉ c·∫£i thi·ªán v·ªõi NLP library)
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if not sentences:
        return [text]
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence_words = sentence.split()
        sentence_length = len(sentence_words)
        
        # N·∫øu c√¢u qu√° d√†i so v·ªõi chunk_size, chia nh·ªè c√¢u ƒë√≥
        if sentence_length > chunk_size:
            if current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_length = 0
            
            # Chia c√¢u d√†i th√†nh c√°c chunk
            words = sentence.split()
            for i in range(0, len(words), chunk_size - overlap):
                chunk_words = words[max(0, i - overlap):i + chunk_size]
                chunks.append(' '.join(chunk_words))
            continue
        
        # Th√™m c√¢u v√†o chunk hi·ªán t·∫°i
        if current_length + sentence_length <= chunk_size:
            current_chunk.append(sentence)
            current_length += sentence_length
        else:
            # L∆∞u chunk hi·ªán t·∫°i
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            
            # B·∫Øt ƒë·∫ßu chunk m·ªõi v·ªõi overlap
            if overlap > 0 and current_chunk:
                # L·∫•y overlap t·ª´ chunk tr∆∞·ªõc
                last_chunk_words = ' '.join(current_chunk).split()
                overlap_words = last_chunk_words[-overlap:] if len(last_chunk_words) >= overlap else last_chunk_words
                current_chunk = [' '.join(overlap_words), sentence]
                current_length = len(overlap_words) + sentence_length
            else:
                current_chunk = [sentence]
                current_length = sentence_length
    
    # Th√™m chunk cu·ªëi c√πng
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    # Lo·∫°i b·ªè c√°c chunk qu√° ng·∫Øn (d∆∞·ªõi 10 t·ª´)
    chunks = [chunk for chunk in chunks if len(chunk.split()) >= 10]
    
    return chunks

def download_and_filter_data():
    """T·∫£i v√† l·ªçc d·ªØ li·ªáu t·ª´ HuggingFace dataset"""
    
    # ========== CONFIGURATION ==========
    target_domains = [
        "Science",
        "Computers_and_Electronics",
        "Internet_and_Telecom",
        "Finance",
        "Law_and_Government",
        "Health",
        "Jobs_and_Education",
        "Travel_and_Transportation"
    ]
    
    # CHUNKING CONFIG
    CHUNK_CONFIGS = {
        "small": {"chunk_size": 256, "overlap": 32, "split_method": "sentences"},
        "medium": {"chunk_size": 512, "overlap": 64, "split_method": "sentences"},
        "large": {"chunk_size": 1024, "overlap": 128, "split_method": "sentences"},
    }
    
    target_domains_set = set(target_domains)
    
    # Gi·ªõi h·∫°n m·ªói domain (tr∆∞·ªõc khi chunking)
    MAX_SAMPLES_PER_DOMAIN = {
        "Science": 3000,
        "Computers_and_Electronics": 2500,
        "Business_and_Industrial": 1500,
        "Internet_and_Telecom": 3000,
        "Finance": 1500,
        "Law_and_Government": 1000,
        "Health": 1000,
        "Jobs_and_Education": 1000,
        "Travel_and_Transportation": 1500
    }
    
    BATCH_SIZE = 512  # S·ªë samples m·ªói file
    OUTPUT_DIR = "./filtered_data"
    
    # Ch·ªçn config chunking
    CHUNKING_CONFIG_NAME = "small"  # small/medium/large
    CHUNK_SIZE = CHUNK_CONFIGS[CHUNKING_CONFIG_NAME]["chunk_size"]
    OVERLAP = CHUNK_CONFIGS[CHUNKING_CONFIG_NAME]["overlap"]
    SPLIT_METHOD = CHUNK_CONFIGS[CHUNKING_CONFIG_NAME]["split_method"]
    
    # ========== CREATE OUTPUT DIRECTORY ==========
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # L∆∞u config
    config = {
        "target_domains": target_domains,
        "max_samples_per_domain": MAX_SAMPLES_PER_DOMAIN,  # Dictionary v·ªõi limit cho t·ª´ng domain
        "chunking_config": CHUNKING_CONFIG_NAME,
        "chunk_size": CHUNK_SIZE,
        "overlap": OVERLAP,
        "split_method": SPLIT_METHOD
    }
    
    with open(os.path.join(OUTPUT_DIR, "config.json"), "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # ========== INITIALIZE COUNTERS ==========
    domain_counter = {domain: 0 for domain in target_domains}
    domain_chunk_counter = {domain: 0 for domain in target_domains}  # ƒê·∫øm chunks
    domain_data = {domain: [] for domain in target_domains}
    total_processed = 0
    total_chunks_created = 0
    
    # ========== LOAD DATASET ==========
    print("üì• ƒêang t·∫£i dataset t·ª´ HuggingFace...")
    ds = load_dataset("VTSNLP/vietnamese_curated_dataset", split="train", streaming=True)
    
    print(f"\nüéØ Target domains: {len(target_domains)}")
    print(f"üìä Max samples per domain:")
    for domain in target_domains:
        print(f"   ‚Ä¢ {domain}: {MAX_SAMPLES_PER_DOMAIN[domain]}")
    print(f"üì¶ Batch size: {BATCH_SIZE}")
    print(f"\nüî™ Chunking Config: {CHUNKING_CONFIG_NAME}")
    print(f"   ‚Ä¢ Chunk size: {CHUNK_SIZE} t·ª´")
    print(f"   ‚Ä¢ Overlap: {OVERLAP} t·ª´")
    print(f"   ‚Ä¢ Split method: {SPLIT_METHOD}")
    
    # ========== PROCESS DATASET ==========
    print("\nüîÑ ƒêang l·ªçc, chunking v√† x·ª≠ l√Ω d·ªØ li·ªáu...")
    
    try:
        for item in tqdm(ds, desc="Processing", unit=" samples"):
            domain = item["domain"]
            
            # Ch·ªâ l·∫•y domains trong target v√† ch∆∞a ƒë·ªß gi·ªõi h·∫°n
            if domain in target_domains_set and domain_counter[domain] < MAX_SAMPLES_PER_DOMAIN[domain]:
                domain_counter[domain] += 1
                total_processed += 1
                
                # T√≠nh ƒë·ªô d√†i vƒÉn b·∫£n g·ªëc
                original_length = len(item["text"].split())
                
                # Chunking vƒÉn b·∫£n
                if SPLIT_METHOD == "sentences":
                    chunks = split_by_sentences(item["text"], CHUNK_SIZE, OVERLAP)
                else:
                    chunks = chunk_text(item["text"], CHUNK_SIZE, OVERLAP)
                
                # Th√™m t·ª´ng chunk v√†o buffer
                for chunk_idx, chunk in enumerate(chunks):
                    chunk_length = len(chunk.split())
                    domain_chunk_counter[domain] += 1
                    total_chunks_created += 1
                    
                    domain_data[domain].append({
                        "text": chunk,
                        "domain": domain,
                        "original_length": original_length,
                        "chunk_length": chunk_length,
                        "chunk_id": chunk_idx,
                        "total_chunks": len(chunks),
                        "original_id": item.get("id", total_processed),
                        "chunking_config": CHUNKING_CONFIG_NAME,
                        "chunk_size": CHUNK_SIZE,
                        "overlap": OVERLAP
                    })
                
                # L∆∞u batch khi ƒë·ªß BATCH_SIZE chunks
                if len(domain_data[domain]) >= BATCH_SIZE:
                    save_batch(domain, domain_data[domain], OUTPUT_DIR)
                    domain_data[domain] = []  # Reset buffer
            
            # D·ª´ng khi t·∫•t c·∫£ domains ƒë√£ ƒë·ªß
            if all(domain_counter[domain] >= MAX_SAMPLES_PER_DOMAIN[domain] for domain in target_domains):
                print("\n‚úÖ ƒê√£ ƒë·ªß samples cho t·∫•t c·∫£ domains!")
                break
                
    except KeyboardInterrupt:
        print("\n‚è∏Ô∏è ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        raise
    
    # ========== SAVE REMAINING DATA ==========
    print("\nüíæ ƒêang l∆∞u c√°c batches c√≤n l·∫°i...")
    for domain in target_domains:
        if domain_data[domain]:
            save_batch(domain, domain_data[domain], OUTPUT_DIR)
    
    # ========== PRINT STATISTICS ==========
    print(f"\n{'='*60}")
    print("üìä TH·ªêNG K√ä CHUNKING")
    print(f"{'='*60}")
    
    print(f"T·ªïng vƒÉn b·∫£n g·ªëc ƒë√£ x·ª≠ l√Ω: {total_processed:,}")
    print(f"T·ªïng chunks ƒë√£ t·∫°o: {total_chunks_created:,}")
    print(f"T·ª∑ l·ªá chunk/ vƒÉn b·∫£n: {total_chunks_created/total_processed:.2f}")
    
    print(f"\nPh√¢n ph·ªëi theo domain (vƒÉn b·∫£n g·ªëc):")
    for domain in target_domains:
        count = domain_counter[domain]
        print(f"  {domain:30s}: {count:5d} samples")
    
    print(f"\nPh√¢n ph·ªëi chunks theo domain:")
    for domain in target_domains:
        count = domain_chunk_counter[domain]
        print(f"  {domain:30s}: {count:5d} chunks")
    
    # ========== SAVE FINAL STATISTICS ==========
    stats = {
        "total_original_processed": total_processed,
        "total_chunks_created": total_chunks_created,
        "avg_chunks_per_doc": total_chunks_created / total_processed if total_processed > 0 else 0,
        "max_per_domain": MAX_SAMPLES_PER_DOMAIN,  # Dictionary v·ªõi limit ri√™ng cho t·ª´ng domain
        "target_domains": target_domains,
        "domain_distribution": domain_counter,
        "chunk_distribution": domain_chunk_counter,
        "chunking_config": {
            "name": CHUNKING_CONFIG_NAME,
            "chunk_size": CHUNK_SIZE,
            "overlap": OVERLAP,
            "split_method": SPLIT_METHOD
        }
    }
    
    stats_file = os.path.join(OUTPUT_DIR, "statistics.json")
    with open(stats_file, "w", encoding="utf-8") as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ ƒê√£ l∆∞u th·ªëng k√™ v√†o: {stats_file}")
    print(f"‚úÖ ƒê√£ l∆∞u config v√†o: {os.path.join(OUTPUT_DIR, 'config.json')}")
    print(f"‚úÖ D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u trong: {OUTPUT_DIR}/")

def save_batch(domain_name, batch_data, output_dir):
    """L∆∞u m·ªôt batch d·ªØ li·ªáu"""
    
    # T·∫°o th∆∞ m·ª•c domain n·∫øu ch∆∞a c√≥
    domain_dir = os.path.join(output_dir, domain_name)
    os.makedirs(domain_dir, exist_ok=True)
    
    # ƒê·∫øm s·ªë batch hi·ªán c√≥
    existing_files = [f for f in os.listdir(domain_dir) 
                     if f.startswith("batch_") and f.endswith('.jsonl')]
    batch_num = len(existing_files)
    
    # T·∫°o t√™n file
    filename = os.path.join(domain_dir, f"batch_{batch_num:03d}.jsonl")
    
    # L∆∞u d·ªØ li·ªáu
    with open(filename, "w", encoding="utf-8") as f:
        for item in batch_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    # T√≠nh th·ªëng k√™ batch
    avg_chunk_length = sum(item["chunk_length"] for item in batch_data) / len(batch_data)
    avg_original_length = sum(item["original_length"] for item in batch_data) / len(batch_data)
    
    print(f"  ‚úì {domain_name}: ƒê√£ l∆∞u {len(batch_data)} chunks v√†o {filename}")
    print(f"    ‚Ä¢ Avg chunk length: {avg_chunk_length:.1f} t·ª´")
    print(f"    ‚Ä¢ Avg original length: {avg_original_length:.1f} t·ª´")
    
    # L∆∞u th√¥ng tin batch
    summary = {
        "domain": domain_name,
        "batch_number": batch_num,
        "chunks": len(batch_data),
        "avg_chunk_length": avg_chunk_length,
        "avg_original_length": avg_original_length,
        "min_chunk_length": min(item["chunk_length"] for item in batch_data),
        "max_chunk_length": max(item["chunk_length"] for item in batch_data)
    }
    
    summary_file = os.path.join(domain_dir, f"batch_{batch_num:03d}_summary.json")
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

def check_and_create_filtered_data():
    """Ki·ªÉm tra v√† t·∫°o d·ªØ li·ªáu n·∫øu ch∆∞a c√≥"""
    
    if not os.path.exists("./filtered_data"):
        print("üìÅ Th∆∞ m·ª•c filtered_data ch∆∞a t·ªìn t·∫°i. B·∫Øt ƒë·∫ßu t·∫°o d·ªØ li·ªáu...")
        download_and_filter_data()
    else:
        # ƒê·∫øm s·ªë file JSONL trong filtered_data
        import glob
        jsonl_files = glob.glob("./filtered_data/*/*.jsonl")
        
        if len(jsonl_files) == 0:
            print("üìÅ Th∆∞ m·ª•c filtered_data t·ªìn t·∫°i nh∆∞ng r·ªóng. B·∫Øt ƒë·∫ßu t·∫°o d·ªØ li·ªáu...")
            download_and_filter_data()
        else:
            # Ki·ªÉm tra config hi·ªán t·∫°i
            config_file = "./filtered_data/config.json"
            if os.path.exists(config_file):
                with open(config_file, "r", encoding="utf-8") as f:
                    config = json.load(f)
                print(f"‚úÖ Th∆∞ m·ª•c filtered_data ƒë√£ t·ªìn t·∫°i v·ªõi config:")
                print(f"   ‚Ä¢ Domains: {len(config['target_domains'])}")
                print(f"   ‚Ä¢ Chunk size: {config.get('chunk_size', 'N/A')}")
                print(f"   ‚Ä¢ Overlap: {config.get('overlap', 'N/A')}")
            else:
                print(f"‚úÖ Th∆∞ m·ª•c filtered_data ƒë√£ t·ªìn t·∫°i v·ªõi {len(jsonl_files)} files")
            
            print("   B·ªè qua b∆∞·ªõc t·∫°o d·ªØ li·ªáu.")
            return True
    
    return False

if __name__ == "__main__":
    print("üöÄ B·∫ÆT ƒê·∫¶U T·∫¢I V√Ä L·ªåC D·ªÆ LI·ªÜU V·ªöI CHUNKING")
    print("="*60)
    
    # Ki·ªÉm tra v√† t·∫°o d·ªØ li·ªáu n·∫øu c·∫ßn
    data_exists = check_and_create_filtered_data()
    
    if data_exists:
        print("\nüìä Th·ªëng k√™ th∆∞ m·ª•c filtered_data:")
        import glob
        jsonl_files = glob.glob("./filtered_data/*/*.jsonl")
        domains = set([os.path.basename(os.path.dirname(f)) for f in jsonl_files])
        
        print(f"  ‚Ä¢ S·ªë domains: {len(domains)}")
        print(f"  ‚Ä¢ S·ªë files: {len(jsonl_files)}")
        print(f"  ‚Ä¢ C√°c domains: {', '.join(sorted(domains))}")
        
        # ƒê·ªçc th·ªëng k√™ t·ªïng
        stats_file = "./filtered_data/statistics.json"
        if os.path.exists(stats_file):
            with open(stats_file, "r", encoding="utf-8") as f:
                stats = json.load(f)
            print(f"  ‚Ä¢ T·ªïng vƒÉn b·∫£n g·ªëc: {stats.get('total_original_processed', 'N/A')}")
            print(f"  ‚Ä¢ T·ªïng chunks: {stats.get('total_chunks_created', 'N/A')}")

import os
os._exit(0)